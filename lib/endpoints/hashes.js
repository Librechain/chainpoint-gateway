const restify = require('restify')
const env = require('../parse-env.js')
const _ = require('lodash')
const utils = require('../utils.js')
const MerkleTools = require('merkle-tools')
const crypto = require('crypto')
const request = require('request')
const async = require('async')
const cpb = require('chainpoint-binary')

// The redis connection used for all redis communication
// This value is set once the connection has been established
let redis = null

// An array of hashes received awaiting aggregation
let HASHES = []

// An dictionary of objects containing hashes submitted to Core
// Each object contains a hash representing the merkle root of a Core aggregation
// as well as a proof data object containing the hashes making up the merkle tree,
// the corresponding hash_ids, and each hash's proof path to the root
let CORE_SUBMISSIONS = {}

// the aggregation frequency
let AGG_FOR_CORE_INTERVAL_MS = 1000

// the proof check frequency
let PROOF_CHECK_INTERVAL_MS = 10000

// The merkle tools object for building trees and generating proof paths
const merkleTools = new MerkleTools()

// Generate a v1 UUID (time-based)
// see: https://github.com/broofa/node-uuid
const uuidv1 = require('uuid/v1')

/**
 * Converts proof path array output from the merkle-tools package
 * to a Chainpoint v3 ops array
 *
 * @param {proof object array} proof - The proof array generated by merkle-tools
 * @param {string} op - The hash type performed throughout merkle tree construction (sha-256, sha-512, sha-256-x2, etc.)
 * @returns {ops object array}
 */
function formatAsChainpointV3Ops (proof, op) {
  proof = proof.map((item) => {
    if (item.left) {
      return { l: item.left }
    } else {
      return { r: item.right }
    }
  })
  let ChainpointV3Ops = []
  for (let x = 0; x < proof.length; x++) {
    ChainpointV3Ops.push(proof[x])
    ChainpointV3Ops.push({ op: op })
  }
  return ChainpointV3Ops
}

/**
 * Generate the values for the 'meta' property in a POST /hashes response.
 *
 * Returns an Object with metadata about a POST /hashes request
 * including a 'timestamp', and hints for estimated time to completion
 * for various operations.
 *
 * @returns {Object}
 */
function generatePostHashesResponseMetadata () {
  let metaDataObj = {}
  let timestamp = new Date()
  metaDataObj.submitted_at = utils.formatDateISO8601NoMs(timestamp)

  // FIXME : Calculate these based on last anchor time and known interval?
  metaDataObj.processing_hints = {
    cal: utils.formatDateISO8601NoMs(utils.addSeconds(timestamp, 10)),
    eth: utils.formatDateISO8601NoMs(utils.addMinutes(timestamp, 41)),
    btc: utils.formatDateISO8601NoMs(utils.addMinutes(timestamp, 61))
  }

  return metaDataObj
}

/**
 * Converts an array of hash strings to a object suitable to
 * return to HTTP clients.
 *
 * @param {string[]} hashes - An array of string hashes to process
 * @returns {Object} An Object with 'meta' and 'hashes' properties
 */
function generatePostHashesResponse (hashes) {
  let lcHashes = utils.lowerCaseHashes(hashes)
  let hashObjects = lcHashes.map((hash) => {
    let hashObj = {}
    hashObj.hash_id = uuidv1()
    hashObj.hash = hash
    return hashObj
  })

  return {
    meta: generatePostHashesResponseMetadata(hashObjects),
    hashes: hashObjects
  }
}

/**
 * POST /hashes handler
 *
 * Expects a JSON body with the form:
 *   {"hashes": ["hash1", "hash2", "hashN"]}
 *
 * The `hashes` key must reference a JSON Array
 * of strings representing each hash to anchor.
 *
 * Each hash must be:
 * - in Hexadecimal form [a-fA-F0-9]
 * - minimum 40 chars long (e.g. 20 byte SHA1)
 * - maximum 128 chars long (e.g. 64 byte SHA512)
 * - an even length string
 */
function postHashesV1 (req, res, next) {
  // validate content-type sent was 'application/json'
  if (req.contentType() !== 'application/json') {
    return next(new restify.InvalidArgumentError('invalid content type'))
  }

  // validate params has parse a 'hashes' key
  if (!req.params.hasOwnProperty('hashes')) {
    return next(new restify.InvalidArgumentError('invalid JSON body, missing hashes'))
  }

  // validate hashes param is an Array
  if (!_.isArray(req.params.hashes)) {
    return next(new restify.InvalidArgumentError('invalid JSON body, hashes is not an Array'))
  }

  // validate hashes param Array has at least one hash
  if (_.size(req.params.hashes) < 1) {
    return next(new restify.InvalidArgumentError('invalid JSON body, hashes Array is empty'))
  }

  // validate hashes param Array is not larger than allowed max length
  if (_.size(req.params.hashes) > env.POST_HASHES_MAX) {
    return next(new restify.InvalidArgumentError(`invalid JSON body, hashes Array max size of ${env.POST_HASHES_MAX} exceeded`))
  }

  // validate hashes are individually well formed
  let containsValidHashes = _.every(req.params.hashes, (hash) => {
    return /^([a-fA-F0-9]{2}){20,64}$/.test(hash)
  })

  if (!containsValidHashes) {
    return next(new restify.InvalidArgumentError('invalid JSON body, invalid hashes present'))
  }

  let responseObj = generatePostHashesResponse(req.params.hashes)

  // store hashes for later aggregation
  HASHES = HASHES.concat(responseObj.hashes)

  res.send(responseObj)
  return next()
}

// Build a merkle tree from HASHES and submit the root to Core
function aggregateForCore () {
  let hashesForTree = HASHES.splice(0)

  if (hashesForTree.length > 0) {
    console.log(`Building aggregation tree with ${hashesForTree.length} hash(es) for this interval`)

    // clear the merkleTools instance to prepare for a new tree
    merkleTools.resetTree()

    // concatenate and hash the hash ids and hash values into new array
    let leaves = hashesForTree.map((hashObj) => {
      let hashIdBuffer = Buffer.from(hashObj.hash_id, 'utf8')
      let hashBuffer = Buffer.from(hashObj.hash, 'hex')
      let concatAndHashBuffer = crypto.createHash('sha256').update(Buffer.concat([hashIdBuffer, hashBuffer])).digest()
      return concatAndHashBuffer
    })

    // Add every hash in hashesForTree to new Merkle tree
    merkleTools.addLeaves(leaves)
    merkleTools.makeTree()

    let treeSize = merkleTools.getLeafCount()

    // Collect and store the merkle root, and proofs in an array to be used for proof generation
    let coreAggregationData = {}
    coreAggregationData.merkle_root = merkleTools.getMerkleRoot().toString('hex')

    let proofData = []
    for (let x = 0; x < treeSize; x++) {
      // push the hash_id and corresponding proof onto the array, inserting the UUID concat/hash step at the beginning
      let proofDataItem = {}
      proofDataItem.hash_id = hashesForTree[x].hash_id
      proofDataItem.hash = hashesForTree[x].hash
      let partialProofPath = merkleTools.getProof(x)
      partialProofPath.unshift({ left: hashesForTree[x].hash_id })
      proofDataItem.partial_proof_path = formatAsChainpointV3Ops(partialProofPath, 'sha-256')
      proofData.push(proofDataItem)
    }
    coreAggregationData.proof_data = proofData

    // submit merkle root to Core
    submitHashToCore([coreAggregationData.merkle_root], env.CHAINPOINT_CORE_API_BASE_URI, (err, result) => {
      if (err) {
        console.error(`Could not submit hash to Core: ${err.message}`)
      } else {
        coreAggregationData.processing_hints = result.processing_hints
        coreAggregationData.core_hash_id = result.hash_id
        coreAggregationData.stage = 'cal'
        // store object for later use when generating proofs
        CORE_SUBMISSIONS[coreAggregationData.core_hash_id] = coreAggregationData
        console.log(`Hash submitted to Core : Core hash_id : ${coreAggregationData.core_hash_id}`)
      }
    })
  }
}
// run aggregateToCore() at a regular interval
setInterval(aggregateForCore, AGG_FOR_CORE_INTERVAL_MS)

// check for proofs from Core for each hash in CORE_SUBMISSIONS
function getProofsFromCore () {
  // assemble array of hashes ready to be checked for new proofs based on stage and processing hints
  let hashIdsToCheck = []
  for (var coreHashId in CORE_SUBMISSIONS) {
    if (CORE_SUBMISSIONS.hasOwnProperty(coreHashId)) {
      let aggData = CORE_SUBMISSIONS[coreHashId]
      // wait for recommended time before checking for btc proofs
      // TODO: Update this to account for eth
      if (aggData.stage === 'btc') {
        let readyTime = Date.parse(aggData.processing_hints[aggData.stage])
        if (readyTime <= Date.now()) hashIdsToCheck.push(coreHashId)
      } else {
        hashIdsToCheck.push(coreHashId)
      }
    }
  }
  // split hashIdsToCheck into batches of 250, to honor max proof query limit
  // TODO: Read max proof query limit from stack's /config
  // TODO: delete any outstanding items with a UUID older than proof expire minutes
  let hashIdsToCheckBatches = []
  while (hashIdsToCheck.length > 0) {
    hashIdsToCheckBatches.push(hashIdsToCheck.splice(0, 250))
  }
  // iterate through each batch, checking for proofs from Core
  async.eachLimit(hashIdsToCheckBatches, 10, (hashIdsToCheck, eachCallback) => {
    checkForProofsByHashIds(hashIdsToCheck, env.CHAINPOINT_CORE_API_BASE_URI, (err, results) => {
      if (err) return eachCallback(err)
      async.eachLimit(results, 10, (checkForProofResult, eachCheckCallback) => {
        if (checkForProofResult.proof) {
          // a proof has been found for this hash_id
          // prepare full proofs by combining the proof with proof data
          let fullProofData = CORE_SUBMISSIONS[checkForProofResult.hash_id].proof_data.map((proofDataItem) => {
            let fullProofItem = {}
            fullProofItem.hash_id = proofDataItem.hash_id
            let fullProof = _.cloneDeep(checkForProofResult.proof)
            fullProof.hash_id = proofDataItem.hash_id
            fullProof.hash = proofDataItem.hash
            // TODO: Update spec to allow hash_id_core, then renable line below
            // fullProof.hash_id_core = checkForProofResult.hash_id
            for (let y = proofDataItem.partial_proof_path.length - 1; y >= 0; y--) {
              fullProof.branches[0].ops.unshift(proofDataItem.partial_proof_path[y])
            }
            fullProofItem.proof = fullProof
            return fullProofItem
          })
          async.eachLimit(fullProofData, 10, (fullProofDataItem, eachFullProofCallback) => {
            // save each to redis
            let fullProofBase64 = cpb.objectToBase64Sync(fullProofDataItem.proof)
            redis.set(fullProofDataItem.hash_id, fullProofBase64, 'EX', env.PROOF_EXPIRE_MINUTES * 60, (err, res) => {
              if (err) return eachFullProofCallback(err)
              return eachFullProofCallback(null)
            })
          }, (err) => {
            if (err) return eachCheckCallback(err)
            // promote stage
            // TODO: account for eth/btc stage, for now assume 0 > cal > btc
            if (CORE_SUBMISSIONS[checkForProofResult.hash_id].stage === 'cal') {
              CORE_SUBMISSIONS[checkForProofResult.hash_id].stage = 'btc'
            } else {
              delete CORE_SUBMISSIONS[checkForProofResult.hash_id]
            }
            return eachCheckCallback(null)
          })
        } else {
          return eachCheckCallback(null)
        }
      }, (err) => {
        if (err) return eachCallback(err)
        return eachCallback(null)
      })
    })
  }, (err) => {
    if (err) console.log(`Error querying for proofs on Core : ${err}`)
  })
}
setInterval(getProofsFromCore, PROOF_CHECK_INTERVAL_MS)

function submitHashToCore (hashArray, baseURI, callback) {
  let options = {
    headers: {
      'Content-Type': 'application/json'
    },
    method: 'POST',
    uri: baseURI + '/hashes',
    body: { hashes: hashArray },
    json: true,
    gzip: true
  }

  request(options, (err, response, responseBody) => {
    if (err) return callback(err)
    if (response.statusCode !== 200) return callback({ message: `Invalid response : ${responseBody.message}` })
    if (!responseBody.meta) return callback({ message: 'Invalid response : missing meta field' })
    if (!responseBody.hashes) return callback({ message: 'Invalid response : missing hashes field' })
    let result = {
      processing_hints: responseBody.meta.processing_hints,
      hash_id: responseBody.hashes[0].hash_id
    }
    return callback(null, result)
  })
}

function checkForProofsByHashIds (hashIdArray, baseURI, callback) {
  let hashIdCSV = hashIdArray.join(',')
  let options = {
    headers: {
      'Content-Type': 'application/json',
      hashids: hashIdCSV
    },
    method: 'GET',
    uri: baseURI + '/proofs',
    json: true,
    gzip: true
  }

  request(options, (err, response, body) => {
    if (err) return callback(err)
    if (response.statusCode !== 200) return callback({ message: 'Invalid response' })

    let proofCheckResults = []

    for (let x = 0; x < body.length; x++) {
      let hashItem = body[x]
      if (hashItem.proof === null) {
        proofCheckResults.push({ hash_id: hashItem.hash_id, proof: null })
      } else {
        let proofObject = null
        try {
          proofObject = cpb.binaryToObjectSync(hashItem.proof)
        } catch (err) {
          proofCheckResults.push({ hash_id: hashItem.hash_id, proof: '' })
        }
        if (proofObject) {
          // a proof has been returned
          // TODO: Determine btc/eth stage, for now assume 0 > cal > btc
          proofCheckResults.push({ hash_id: hashItem.hash_id, proof: proofObject })
        }
      }
    }

    return callback(null, proofCheckResults)
  })
}

module.exports = {
  postHashesV1: postHashesV1,
  generatePostHashesResponse: generatePostHashesResponse,
  setRedis: (redisClient) => { redis = redisClient }
}
