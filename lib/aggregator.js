// load environment variables
const env = require('./parse-env.js')

const MerkleTools = require('merkle-tools')
const crypto = require('crypto')
const request = require('request')
const async = require('async')
const cpb = require('chainpoint-binary')
const _ = require('lodash')
const utils = require('./utils.js')
const uuidTime = require('uuid-time')

// The redis connection used for all redis communication
// This value is set once the connection has been established
let redis = null

// An array of hashes received awaiting aggregation
let HASHES = []

// An dictionary of objects containing hashes submitted to Core
// Each object contains a hash representing the merkle root of a Core aggregation
// as well as a proof data object containing the hashes making up the merkle tree,
// the corresponding hash_ids, and each hash's proof path to the root
let CORE_SUBMISSIONS = {}

// the aggregation frequency
let AGG_FOR_CORE_INTERVAL_MS = 1000

// the proof check frequency
let PROOF_CHECK_INTERVAL_MS = 10000

// The merkle tools object for building trees and generating proof paths
const merkleTools = new MerkleTools()

/**
 * Converts proof path array output from the merkle-tools package
 * to a Chainpoint v3 ops array
 *
 * @param {proof object array} proof - The proof array generated by merkle-tools
 * @param {string} op - The hash type performed throughout merkle tree construction (sha-256, sha-512, sha-256-x2, etc.)
 * @returns {ops object array}
 */
function formatAsChainpointV3Ops (proof, op) {
  proof = proof.map((item) => {
    if (item.left) {
      return { l: item.left }
    } else {
      return { r: item.right }
    }
  })
  let ChainpointV3Ops = []
  for (let x = 0; x < proof.length; x++) {
    ChainpointV3Ops.push(proof[x])
    ChainpointV3Ops.push({ op: op })
  }
  return ChainpointV3Ops
}

// Build a merkle tree from HASHES and submit the root to Core
function aggregateAndSendToCore () {
  let hashesForTree = HASHES.splice(0)

  if (hashesForTree.length > 0) {
    console.log(`Building aggregation tree with ${hashesForTree.length} hash(es) for this interval`)

    // clear the merkleTools instance to prepare for a new tree
    merkleTools.resetTree()

    // concatenate and hash the hash ids and hash values into new array
    let leaves = hashesForTree.map((hashObj) => {
      let hashIdBuffer = Buffer.from(hashObj.hash_id, 'utf8')
      let hashBuffer = Buffer.from(hashObj.hash, 'hex')
      let concatAndHashBuffer = crypto.createHash('sha256').update(Buffer.concat([hashIdBuffer, hashBuffer])).digest()
      return concatAndHashBuffer
    })

    // Add every hash in hashesForTree to new Merkle tree
    merkleTools.addLeaves(leaves)
    merkleTools.makeTree()

    let treeSize = merkleTools.getLeafCount()

    // Collect and store the merkle root, and proofs in an array to be used for proof generation
    let coreAggregationData = {}
    coreAggregationData.merkle_root = merkleTools.getMerkleRoot().toString('hex')

    let proofData = []
    for (let x = 0; x < treeSize; x++) {
      // push the hash_id and corresponding proof onto the array, inserting the UUID concat/hash step at the beginning
      let proofDataItem = {}
      proofDataItem.hash_id = hashesForTree[x].hash_id
      proofDataItem.hash = hashesForTree[x].hash
      let partialProofPath = merkleTools.getProof(x)
      partialProofPath.unshift({ left: hashesForTree[x].hash_id })
      proofDataItem.partial_proof_path = formatAsChainpointV3Ops(partialProofPath, 'sha-256')
      proofData.push(proofDataItem)
    }
    coreAggregationData.proof_data = proofData

    // submit merkle root to Core
    submitHashToCore([coreAggregationData.merkle_root], env.CHAINPOINT_CORE_API_BASE_URI, (err, result) => {
      if (err) {
        console.error(`Could not submit hash to Core: ${err.message}`)
      } else {
        coreAggregationData.processing_hints = result.processing_hints
        coreAggregationData.hash_id_core = result.hash_id
        coreAggregationData.stage = 'cal'
        // store object for later use when generating proofs
        CORE_SUBMISSIONS[coreAggregationData.hash_id_core] = coreAggregationData
        console.log(`Hash submitted to Core : hash_id_core : ${coreAggregationData.hash_id_core}`)
      }
    })
  }
}
// run aggregateToCore() at a regular interval
setInterval(aggregateAndSendToCore, AGG_FOR_CORE_INTERVAL_MS)

// check for proofs from Core for each hash in CORE_SUBMISSIONS
function checkForProofsFromCore () {
  // assemble array of hashes ready to be checked for new proofs based on stage and processing hints
  let hashIdsToCheck = []
  for (var hashIdCore in CORE_SUBMISSIONS) {
    if (CORE_SUBMISSIONS.hasOwnProperty(hashIdCore)) {
      let aggData = CORE_SUBMISSIONS[hashIdCore]
      // wait for recommended time before checking for btc proofs
      // TODO: Update this to account for eth
      if (aggData.stage === 'btc') {
        let readyTime = Date.parse(aggData.processing_hints[aggData.stage])
        if (readyTime <= Date.now()) hashIdsToCheck.push(hashIdCore)
      } else {
        hashIdsToCheck.push(hashIdCore)
      }
    }
  }
  // split hashIdsToCheck into batches of 250, to honor max proof query limit
  // TODO: Read max proof query limit from stack's /config
  // TODO: delete any outstanding items with a UUID older than proof expire minutes
  let hashIdsToCheckBatches = []
  while (hashIdsToCheck.length > 0) {
    hashIdsToCheckBatches.push(hashIdsToCheck.splice(0, 250))
  }
  // iterate through each batch, checking for proofs from Core
  async.eachLimit(hashIdsToCheckBatches, 10, (hashIdsToCheck, eachCallback) => {
    checkForProofsByHashIds(hashIdsToCheck, env.CHAINPOINT_CORE_API_BASE_URI, (err, results) => {
      if (err) return eachCallback(err)
      async.eachLimit(results, 10, (checkForProofResult, eachCheckCallback) => {
        if (checkForProofResult.proof) {
          // a proof has been found for this hash_id
          // prepare full proofs by combining the proof with proof data
          let fullProofData = CORE_SUBMISSIONS[checkForProofResult.hash_id].proof_data.map((proofDataItem) => {
            let fullProofItem = {}
            fullProofItem.hash_id = proofDataItem.hash_id
            let fullProof = _.cloneDeep(checkForProofResult.proof)
            fullProof.hash_id_node = proofDataItem.hash_id
            fullProof.hash_submitted_node_at = utils.formatDateISO8601NoMs(new Date(uuidTime.v1(fullProof.hash_id_node)))
            fullProof.hash = proofDataItem.hash
            for (let y = proofDataItem.partial_proof_path.length - 1; y >= 0; y--) {
              fullProof.branches[0].ops.unshift(proofDataItem.partial_proof_path[y])
            }
            fullProofItem.proof = fullProof
            return fullProofItem
          })
          async.eachLimit(fullProofData, 10, (fullProofDataItem, eachFullProofCallback) => {
            // save each to redis
            let fullProofBase64 = cpb.objectToBase64Sync(fullProofDataItem.proof)
            redis.set(fullProofDataItem.hash_id, fullProofBase64, 'EX', env.PROOF_EXPIRE_MINUTES * 60, (err, res) => {
              if (err) return eachFullProofCallback(err)
              return eachFullProofCallback(null)
            })
          }, (err) => {
            if (err) return eachCheckCallback(err)
            // promote stage
            // TODO: account for eth/btc stage, for now assume 0 > cal > btc
            if (CORE_SUBMISSIONS[checkForProofResult.hash_id].stage === 'cal') {
              CORE_SUBMISSIONS[checkForProofResult.hash_id].stage = 'btc'
            } else {
              delete CORE_SUBMISSIONS[checkForProofResult.hash_id]
            }
            return eachCheckCallback(null)
          })
        } else {
          return eachCheckCallback(null)
        }
      }, (err) => {
        if (err) return eachCallback(err)
        return eachCallback(null)
      })
    })
  }, (err) => {
    if (err) console.error(`Error querying for proofs on Core : ${err}`)
  })
}
setInterval(checkForProofsFromCore, PROOF_CHECK_INTERVAL_MS)

function submitHashToCore (hashArray, baseURI, callback) {
  let options = {
    headers: {
      'Content-Type': 'application/json'
    },
    method: 'POST',
    uri: baseURI + '/hashes',
    body: { hashes: hashArray },
    json: true,
    gzip: true
  }

  request(options, (err, response, responseBody) => {
    if (err) return callback(err)
    if (response.statusCode !== 200) return callback({ message: `Invalid response : ${responseBody.message}` })
    if (!responseBody.meta) return callback({ message: 'Invalid response : missing meta field' })
    if (!responseBody.hashes) return callback({ message: 'Invalid response : missing hashes field' })
    let result = {
      processing_hints: responseBody.meta.processing_hints,
      hash_id: responseBody.hashes[0].hash_id
    }
    return callback(null, result)
  })
}

function checkForProofsByHashIds (hashIdArray, baseURI, callback) {
  let hashIdCSV = hashIdArray.join(',')
  let options = {
    headers: {
      'Content-Type': 'application/json',
      hashids: hashIdCSV
    },
    method: 'GET',
    uri: baseURI + '/proofs',
    json: true,
    gzip: true
  }

  request(options, (err, response, body) => {
    if (err) return callback(err)
    if (response.statusCode !== 200) return callback({ message: 'Invalid response' })

    let proofCheckResults = []

    for (let x = 0; x < body.length; x++) {
      let hashItem = body[x]
      if (hashItem.proof === null) {
        proofCheckResults.push({ hash_id: hashItem.hash_id, proof: null })
      } else {
        let proofObject = null
        try {
          proofObject = cpb.binaryToObjectSync(hashItem.proof)
        } catch (err) {
          proofCheckResults.push({ hash_id: hashItem.hash_id, proof: '' })
        }
        if (proofObject) {
          // a proof has been returned
          // TODO: Determine btc/eth stage, for now assume 0 > cal > btc
          proofCheckResults.push({ hash_id: hashItem.hash_id, proof: proofObject })
        }
      }
    }

    return callback(null, proofCheckResults)
  })
}

module.exports = {
  addHashes: (hashArray) => { HASHES = HASHES.concat(hashArray) },
  setRedis: (redisClient) => { redis = redisClient }
}
